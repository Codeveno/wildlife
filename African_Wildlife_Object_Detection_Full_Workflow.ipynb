{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to Dataset Exploration\n",
    "\n",
    "In this section, we will:\n",
    "\n",
    "Load the African Wildlife dataset.\n",
    "\n",
    "Explore the dataset structure.\n",
    "\n",
    "Visualize sample images with bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install matplotlib and opencv-python\n",
    "%pip install matplotlib opencv-python\n",
    "\n",
    "# Import libraries\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define dataset paths\n",
    "train_image_dir = \"data/african_wildlife/train/images\"\n",
    "train_label_dir = \"data/african_wildlife/train/labels\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to visualize images with bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize images with bounding boxes\n",
    "def plot_image_with_boxes(image_path, label_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB for matplotlib\n",
    "    height, width, _ = image.shape\n",
    "    \n",
    "    with open(label_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            class_id, x_center, y_center, w, h = map(float, line.split())\n",
    "            # Convert YOLO format to pixel coordinates\n",
    "            x_center *= width\n",
    "            y_center *= height\n",
    "            w *= width\n",
    "            h *= height\n",
    "            x_min = int(x_center - w / 2)\n",
    "            y_min = int(y_center - h / 2)\n",
    "            x_max = int(x_center + w / 2)\n",
    "            y_max = int(y_center + h / 2)\n",
    "            \n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (255, 0, 0), 2)\n",
    "    \n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Image: {os.path.basename(image_path)}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize a few samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_images = os.listdir(train_image_dir)[:3]  # First 3 images\n",
    "for img in sample_images:\n",
    "    img_path = os.path.join(train_image_dir, img)\n",
    "    label_path = os.path.join(train_label_dir, img.replace(\".jpg\", \".txt\"))\n",
    "    plot_image_with_boxes(img_path, label_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset Verification\n",
    "\n",
    "We need to ensure that:\n",
    "\n",
    "Every image has a corresponding label file.\n",
    "\n",
    "The annotations are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset integrity\n",
    "def verify_dataset(image_dir, label_dir):\n",
    "    images = set(os.listdir(image_dir))\n",
    "    labels = set(os.listdir(label_dir))\n",
    "    \n",
    "    missing_labels = []\n",
    "    for img in images:\n",
    "        label_file = img.replace(\".jpg\", \".txt\")\n",
    "        if label_file not in labels:\n",
    "            missing_labels.append(img)\n",
    "    \n",
    "    if missing_labels:\n",
    "        print(f\"Missing labels for {len(missing_labels)} images.\")\n",
    "    else:\n",
    "        print(\"All images have corresponding labels.\")\n",
    "\n",
    "verify_dataset(train_image_dir, train_label_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dataset Preprocessing\n",
    "In this section, we will:\n",
    "\n",
    "Resize images to a consistent resolution (e.g., 640x640).\n",
    "\n",
    "Normalize pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize images and save to a new directory\n",
    "def resize_images(image_dir, output_dir, size=(640, 640)):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for img in os.listdir(image_dir):\n",
    "        img_path = os.path.join(image_dir, img)\n",
    "        image = cv2.imread(img_path)\n",
    "        resized_image = cv2.resize(image, size)\n",
    "        cv2.imwrite(os.path.join(output_dir, img), resized_image)\n",
    "\n",
    "# Resize training images\n",
    "resize_images(train_image_dir, \"data/african_wildlife/train/images_resized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ultralytics package\n",
    "%pip install ultralytics\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a pre-trained YOLOv8 model\n",
    "model = YOLO(\"yolov8n.pt\")  # Use 'yolov8s.pt' or 'yolov8m.pt' for larger models\n",
    "\n",
    "# Train the model\n",
    "results = model.train(data=\"data.yaml\", epochs=50, imgsz=640, batch=16, name=\"african_wildlife_yolov8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Training\n",
    "In this section, we will:\n",
    "\n",
    "Load the preprocessed dataset.\n",
    "\n",
    "Train a YOLOv8 model on the African Wildlife dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Evaluation\n",
    "In this section, we will:\n",
    "\n",
    "Evaluate the trained model on the validation set.\n",
    "\n",
    "Compute key metrics like mAP, precision, and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the pre-trained YOLOv8 model\n",
    "model = YOLO(\"C:/Users/mophi/Desktop/wildlife-conservation-system/yolov8n.pt\")\n",
    "# Evaluate on the validation set\n",
    "metrics = model.val()  # Use 'best_model.val()' if you loaded 'best.pt'\n",
    "\n",
    "# Print key metrics\n",
    "print(f\"mAP50: {metrics.box.map50}\")\n",
    "print(f\"mAP50-95: {metrics.box.map}\")\n",
    "print(f\"Precision: {metrics.box.precision}\")\n",
    "print(f\"Recall: {metrics.box.recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Testing and Deployment\n",
    "In this section, we will:\n",
    "\n",
    "Test the model on new images.\n",
    "\n",
    "Export the model for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on new images\n",
    "test_images = [\"path/to/test_image1.jpg\", \"path/to/test_image2.jpg\"]  # Add your test images here\n",
    "\n",
    "for img_path in test_images:\n",
    "    best_model = YOLO(\"yolov8n.pt\")\n",
    "    results = best_model(img_path)\n",
    "    for result in results:\n",
    "        result.show()  \n",
    "        print(f\"Detected objects: {result.boxes.cls}\")  \n",
    "\n",
    "# Export the model to ONNX format\n",
    "best_model.export(format=\"onnx\")\n",
    "print(\"Model exported to ONNX format for deployment.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
